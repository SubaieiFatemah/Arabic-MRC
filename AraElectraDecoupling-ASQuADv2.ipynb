{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import shutil\r\n",
        "from collections import Counter\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from transformers import AutoTokenizer, ElectraForQuestionAnswering, DataCollatorWithPadding,BertModel, ElectraForSequenceClassification, ElectraModel\r\n",
        "from Preprocess.arabertpreprocess import ArabertPreprocessor\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import csv\r\n",
        "torch.manual_seed(3407)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "<torch._C.Generator at 0x7fa784779f70>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1656592275863
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_end_index(answer, context):\r\n",
        "  ## 1 if span match the context 0 otherwise\r\n",
        "  text = answer['text']\r\n",
        "  start_idx = answer['answer_start']\r\n",
        "  end_idx = start_idx + len(text)\r\n",
        "  answer['answer_end'] = end_idx\r\n",
        "  if text == context[start_idx:end_idx]:\r\n",
        "    answer['answer_end'] = end_idx\r\n",
        "    return False\r\n",
        "  for i in range(1,3):\r\n",
        "    if text == context[start_idx-i:end_idx-i]:\r\n",
        "      answer['answer_end']= end_idx-1\r\n",
        "      answer['answer_start'] = start_idx-1\r\n",
        "      return False\r\n",
        "  return True"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656592276101
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def arabert_preprocess(context,question, answer, arabert_prep):\r\n",
        "    answer['text'] = arabert_prep.preprocess(answer['text'])\r\n",
        "    context = arabert_prep.preprocess(context)\r\n",
        "    question = arabert_prep.preprocess(question)\r\n",
        "    res = context.find(answer['text'])\r\n",
        "    if res !=-1:\r\n",
        "        answer['answer_start'] = res\r\n",
        "    return context, question, answer, res"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656592276517
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Read_AAQAD(path,arabert_prep):\r\n",
        "  contexts =[]\r\n",
        "  answers =[]\r\n",
        "  questions =[]\r\n",
        "  IDs= []\r\n",
        "  plausible = []\r\n",
        "  cnt = 0\r\n",
        "  with open(path) as f:\r\n",
        "    aaqad_dict = json.load(f)\r\n",
        "    for article in aaqad_dict['data']:\r\n",
        "      for passage in article['paragraphs']:\r\n",
        "        context = passage['context']\r\n",
        "        for qa in passage['qas']:\r\n",
        "          question = qa['question']\r\n",
        "          if 'plausible_answers' in qa.keys():# there is two cases if the question have no answer then use plausible answer\r\n",
        "            access = 'plausible_answers'\r\n",
        "            plausible.append(True)\r\n",
        "          else:\r\n",
        "            access = 'answers'\r\n",
        "            plausible.append(False)\r\n",
        "          for answer in qa[access]:\r\n",
        "            context,question, answer, res =  arabert_preprocess(context,question, answer, arabert_prep)\r\n",
        "            #if res==-1:\r\n",
        "            #  cnt+=1\r\n",
        "            #  continue\r\n",
        "            flag = add_end_index(answer, context) #if false dont add the \r\n",
        "            cnt =cnt + flag\r\n",
        "            flag = False\r\n",
        "            if not flag:\r\n",
        "              contexts.append(context)\r\n",
        "              answers.append(answer)\r\n",
        "              questions.append(question)\r\n",
        "              IDs.append(int(qa['id']))\r\n",
        "  return contexts,questions,answers,plausible,IDs"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656592276736
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_ids(path):\r\n",
        "  #IDs need to be fixed for evaluating purposes\r\n",
        "    a_file = open(path, \"r\")\r\n",
        "    json_object = json.load(a_file)\r\n",
        "    a_file.close()\r\n",
        "    idx_cnt = 1\r\n",
        "    for article in json_object['data']:\r\n",
        "      for passage in article['paragraphs']:\r\n",
        "        context = passage['context']\r\n",
        "        for qa in passage['qas']:\r\n",
        "            qa['id'] = str(idx_cnt)\r\n",
        "            idx_cnt = idx_cnt + 1\r\n",
        "    a_file = open(path, \"w\")\r\n",
        "    json.dump(json_object, a_file)\r\n",
        "    a_file.close()"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656592276968
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"araelectra-base-discriminator\"\r\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name)\r\n",
        "fix_ids('Data/asquadv2-train.json')\r\n",
        "fix_ids('Data/asquadv2-val.json')\r\n",
        "fix_ids('Data/asquadv2-test.json')\r\n",
        "asquad_train_contexts, asquad_train_questions, asquad_train_answers,asquad_train_plausible, asquad_train_ids = Read_AAQAD('Data/asquadv2-train.json', arabert_prep)\r\n",
        "asquad_val_contexts, asquad_val_questions, asquad_val_answers,asquad_val_plausible, asquad_val_ids = Read_AAQAD('Data/asquadv2-val.json', arabert_prep)\r\n",
        "asquad_test_contexts, asquad_test_questions, asquad_test_answers,asquad_test_plausible, asquad_test_ids = Read_AAQAD('Data/asquadv2-test.json', arabert_prep)\r\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656592377162
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(asquad_test_answers)+len(asquad_train_answers)+len(asquad_val_answers)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "96051"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656592385385
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(asquad_test_answers))\r\n",
        "print(len(asquad_val_answers))\r\n",
        "print(len(asquad_train_answers))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "9606\n9605\n76840\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656592410726
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(asquad_test_plausible)/len(asquad_test_plausible) #shows the percentage of not answerd questions"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "0.452841973766396"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656592521900
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(asquad_train_ids)==len(asquad_train_ids)*(len(asquad_train_ids)+1)/2)\r\n",
        "print(sum(asquad_val_ids)==len(asquad_val_ids)*(len(asquad_val_ids)+1)/2)\r\n",
        "print(sum(asquad_test_ids)==len(asquad_test_ids)*(len(asquad_test_ids)+1)/2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "True\nTrue\nTrue\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525149696
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answered_feat(contexts, questions, answers, plausible):\r\n",
        "    new_contexts, new_questions, new_answers = [], [], []\r\n",
        "    for i in range(len(answers)):\r\n",
        "        if plausible[i] == False:\r\n",
        "            new_contexts.append(contexts[i])\r\n",
        "            new_questions.append(questions[i])\r\n",
        "            new_answers.append(answers[i])\r\n",
        "    return new_contexts, new_questions, new_answers\r\n",
        "span_train_contexts, span_train_questions, span_train_answers = get_answered_feat(asquad_train_contexts, asquad_train_questions, asquad_train_answers, asquad_train_plausible)   "
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525150028
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(span_train_contexts), len(asquad_train_contexts))\r\n",
        "print(sum(asquad_test_plausible))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "42042 76840\n4350\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525150332
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the tokenizer\r\n",
        "model_name =  \"aubmindlab/araelectra-base-discriminator\"\r\n",
        "araelectra_tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case=False)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525155510
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = araelectra_tokenizer(asquad_train_questions, asquad_train_contexts, truncation = True )\r\n",
        "span_train_encodings = araelectra_tokenizer(span_train_questions, span_train_contexts, truncation=True)\r\n",
        "val_encodings = araelectra_tokenizer(asquad_val_questions, asquad_val_contexts, truncation=True, return_offsets_mapping=True)\r\n",
        "test_encodings = araelectra_tokenizer(asquad_test_questions, asquad_test_contexts, truncation=True,  return_offsets_mapping=True)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525175701
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_offset = val_encodings['offset_mapping']\r\n",
        "del val_encodings['offset_mapping']\r\n",
        "test_offset = test_encodings['offset_mapping']\r\n",
        "del test_encodings['offset_mapping']"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525175907
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_ids_to_idx = {k:i for i,k in enumerate(asquad_val_ids)}\r\n",
        "test_ids_to_idx = {k:i for i,k in enumerate(asquad_test_ids)}\r\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525176145
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index_to_token_position(encodings , answers):\r\n",
        "  start_positions = list()\r\n",
        "  end_positions = list()\r\n",
        "  for i in range(len(answers)):\r\n",
        "    start_positions.append(encodings.char_to_token(i, answers[i]['answer_start'], 1))\r\n",
        "    end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'], 1))\r\n",
        "    #if context truncated\r\n",
        "    if start_positions[-1] is None: \r\n",
        "      start_positions[-1] = araelectra_tokenizer.model_max_length\r\n",
        "    #if end index is space\r\n",
        "    itt = 1\r\n",
        "    while end_positions[-1] is None: \r\n",
        "      end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end']-itt, 1)\r\n",
        "      itt = itt + 1 \r\n",
        "  encodings.update({'start_positions': torch.tensor(start_positions), 'end_positions': torch.tensor(end_positions)})\r\n",
        "  encodings['start_positions'] = encodings['start_positions'].view(len(answers), 1)\r\n",
        "  encodings['end_positions'] = encodings['end_positions'].view(len(answers), 1)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525176482
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_token_position(span_train_encodings, span_train_answers)\r\n",
        "index_to_token_position(val_encodings, asquad_val_answers)\r\n",
        "index_to_token_position(test_encodings, asquad_test_answers)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525176778
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_weights_labels_tensors(encodings, plausible):\r\n",
        "  plausible = torch.tensor(plausible)\r\n",
        "  weights = torch.ones(plausible.shape)\r\n",
        "  no_ans = torch.ones(plausible.shape)\r\n",
        "  weights[plausible==False]=2.0\r\n",
        "  no_ans[plausible==False]=0.0\r\n",
        "  weights = weights.view(-1,1)\r\n",
        "  no_ans = no_ans.view(-1,1)\r\n",
        "  encodings.update({'weights':weights, 'no_ans':no_ans})"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525177035
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "add_weights_labels_tensors(train_encodings, asquad_train_plausible)\r\n",
        "add_weights_labels_tensors(val_encodings, asquad_val_plausible)\r\n",
        "add_weights_labels_tensors(test_encodings, asquad_test_plausible)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525177302
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_encodings['IDs'] = asquad_val_ids\r\n",
        "test_encodings['IDs'] = asquad_test_ids"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525177660
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_encodings.keys())\r\n",
        "print(val_encodings.keys())\r\n",
        "print(test_encodings.keys())\r\n",
        "print(span_train_encodings.keys())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'weights', 'no_ans'])\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions', 'weights', 'no_ans', 'IDs'])\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions', 'weights', 'no_ans', 'IDs'])\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525177953
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and DataLoader"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from tqdm import tqdm"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525178299
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AqadDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, encodings):\r\n",
        "        self.encodings = encodings\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.encodings.input_ids)\r\n",
        "\r\n",
        "cls_train_dataset = AqadDataset(train_encodings)\r\n",
        "span_train_dataset = AqadDataset(span_train_encodings)\r\n",
        "val_dataset = AqadDataset(val_encodings)\r\n",
        "test_dataset = AqadDataset(test_encodings)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525178521
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(araelectra_tokenizer)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525178846
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_train_loader = DataLoader(cls_train_dataset, batch_size=8, shuffle= True, collate_fn= data_collator)\r\n",
        "span_train_loader = DataLoader(span_train_dataset, batch_size=8, shuffle= True, collate_fn = data_collator)\r\n",
        "val_loader = DataLoader(val_dataset, batch_size = 8, shuffle = True, collate_fn = data_collator)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size = 8, shuffle = True, collate_fn = data_collator)\r\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525179202
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\r\n",
        "    \"\"\"\r\n",
        "    state: checkpoint to save\r\n",
        "    is_best: is this the best checkpoint; min validation loss\r\n",
        "    checkpoint_path: path to save checkpoint\r\n",
        "    best_model_path: path to save best checkpoint\r\n",
        "    \"\"\"\r\n",
        "    f_path = checkpoint_path\r\n",
        "    # save checkpoint data to the path given, checkpoint_path\r\n",
        "    torch.save(state, f_path)\r\n",
        "    # if it is a best model, min validation loss\r\n",
        "    if is_best:\r\n",
        "        best_fpath = best_model_path\r\n",
        "        # copy that checkpoint file to best path given, best_model_path\r\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525179583
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\r\n",
        "    \"\"\"\r\n",
        "    checkpoint_path: path to saved checkpoint\r\n",
        "    model: model to load checkpoint parameters into       \r\n",
        "    optimizer: optimizer defined in previous training\r\n",
        "    \"\"\"\r\n",
        "    # load check point\r\n",
        "    checkpoint = torch.load(checkpoint_fpath)\r\n",
        "    # initialize state_dict from checkpoint to model\r\n",
        "    model.load_state_dict(checkpoint['state_dict'])\r\n",
        "    # initialize optimizer from checkpoint to optimizer\r\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\r\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\r\n",
        "    results = checkpoint['result_dict']\r\n",
        "    # return model, optimizer, epoch value, min validation loss \r\n",
        "    return model, optimizer, checkpoint['epoch'], results"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525179932
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def order_exp(base_path, exp_name):\r\n",
        "  exp_path = os.path.join(base_path, exp_name)\r\n",
        "  if not os.path.exists(exp_path):\r\n",
        "    os.mkdir(exp_path)\r\n",
        "  curr_ckp_path = os.path.join(exp_path,'curr.pt')\r\n",
        "  best_ckp_path = os.path.join(exp_path, 'best.pt')\r\n",
        "  return curr_ckp_path, best_ckp_path, exp_path"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525180298
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification train and evaluation "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cls_eval(model, data_loader, exp_path, train_loss):\r\n",
        "    model.eval()\r\n",
        "    total_acc = 0\r\n",
        "    total_pred = None\r\n",
        "    total_IDs = None\r\n",
        "    soft = torch.nn.Softmax(dim=1)\r\n",
        "    with open(os.path.join(exp_path,'preds.txt'),'w') as f:\r\n",
        "        pass\r\n",
        "    for batch in data_loader:\r\n",
        "      tokens = batch['input_ids'].to(device)\r\n",
        "      masks = batch['attention_mask'].to(device)\r\n",
        "      tokens_type = batch['token_type_ids'].to(device)\r\n",
        "      IDs = batch['IDs'].to(device)\r\n",
        "      gt_no_ans = batch['no_ans'].to(device)\r\n",
        "      output = model(tokens, masks, tokens_type)\r\n",
        "      pred = output.logits.view(masks.shape[0],2,)\r\n",
        "      with open(os.path.join(exp_path,'preds.txt'),'a') as f:\r\n",
        "        preds_soft = soft(pred)[:,1]\r\n",
        "        IDs\r\n",
        "        for i in range(preds_soft.shape[0]):\r\n",
        "            f.write(f\"{IDs[i].item()},{preds_soft[i].item()}\\n\")\r\n",
        "        \r\n",
        "      pred = torch.argmax(pred, dim=1)\r\n",
        "      target = batch['no_ans'].to(device).view(masks.shape[0],)\r\n",
        "      total_acc += torch.sum(target==pred)\r\n",
        "\r\n",
        "    total_acc = total_acc/ val_dataset.__len__()\r\n",
        "    res_dict = {'acc':total_acc.item()*100, 'train_loss':train_loss}\r\n",
        "    if exp_path:\r\n",
        "        log_path = os.path.join(exp_path,'res.csv')\r\n",
        "        if not os.path.exists(log_path):\r\n",
        "            with open(log_path,'w') as f:\r\n",
        "                writer = csv.DictWriter(f, fieldnames=res_dict.keys())\r\n",
        "                writer.writeheader()\r\n",
        "        with open(log_path, 'a') as f:\r\n",
        "            writer = csv.DictWriter(f, fieldnames=res_dict.keys())\r\n",
        "            #writer.writeheader()\r\n",
        "            writer.writerow(res_dict)\r\n",
        "    model.train()\r\n",
        "    return res_dict\r\n"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525180597
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cls_train(model,start_epoch, num_epochs, optimizer,max_acc, train_loader, val_loader, log, exp_name):\r\n",
        "  curr_ckp_path, best_ckp_path, exp_path = order_exp('Runs/AraElectraDecoupledAsquadv2/train/cls', exp_name)\r\n",
        "  model.train()\r\n",
        "  cls_pred = None\r\n",
        "  for epoch in range(start_epoch,num_epochs):\r\n",
        "    total_loss = 0.0\r\n",
        "    loop = tqdm(train_loader, leave=True)\r\n",
        "    for batch_idx, batch in enumerate(loop):\r\n",
        "      tokens = batch['input_ids'].to(device)\r\n",
        "      masks = batch['attention_mask'].to(device)\r\n",
        "      tokens_type = batch['token_type_ids'].to(device)\r\n",
        "      weights = batch['weights'].to(device)\r\n",
        "      output = model(tokens, masks, tokens_type)\r\n",
        "      pred = output.logits.view(masks.shape[0],2,)\r\n",
        "      target = batch['no_ans'].type(torch.LongTensor)\r\n",
        "      target = target.to(device)\r\n",
        "      loss = cls_criterion(pred, target.view(masks.shape[0],))\r\n",
        "      loss = loss*(weights.view(masks.shape[0],))\r\n",
        "      loss = torch.mean(loss)\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      optimizer.zero_grad()\r\n",
        "      total_loss = total_loss + ((1 / (batch_idx + 1)) * (loss.item() - total_loss)) \r\n",
        "      loop.set_description(f'Epoch {epoch}')\r\n",
        "      loop.set_postfix(loss=loss.item())\r\n",
        "\r\n",
        "    result_dict= cls_eval(model, val_loader,exp_path,total_loss )\r\n",
        "    checkpoint = {\r\n",
        "            'epoch': epoch + 1,\r\n",
        "            'result_dict':result_dict,\r\n",
        "            'state_dict': model.state_dict(),\r\n",
        "            'optimizer': optimizer.state_dict(),\r\n",
        "        }\r\n",
        "    curr_acc = result_dict['acc']\r\n",
        "    if curr_acc>=max_acc:\r\n",
        "      max_acc = curr_acc\r\n",
        "      save_ckp(checkpoint, True, curr_ckp_path, best_ckp_path)\r\n",
        "    else:\r\n",
        "      save_ckp(checkpoint, False, curr_ckp_path, best_ckp_path)\r\n",
        "    print(result_dict)\r\n",
        "  return model\r\n"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525180800
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Cls_AraElectra = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=2)\r\n",
        "QA_AraElectra = ElectraForQuestionAnswering.from_pretrained(model_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656477729575
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze(Electra, count=None):\r\n",
        "    if count is not None:\r\n",
        "\t      # We freeze here the embeddings of the model\r\n",
        "        for param in Electra.embeddings.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "\r\n",
        "        if count != -1:\r\n",
        "\t          # if freeze_layer_count == -1, we only freeze the embedding layer\r\n",
        "\t          # otherwise we freeze the first `freeze_layer_count` encoder layers\r\n",
        "            for layer in Electra.encoder.layer[:count]:\r\n",
        "                for param in layer.parameters():\r\n",
        "                    param.requires_grad = False\r\n",
        "    print(sum(p.numel() for p in Electra.parameters()), sum(p.numel() for p in Electra.parameters() if p.requires_grad))"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656516162718
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freeze(Cls_AraElectra.electra,6)\r\n",
        "freeze(QA_AraElectra.electra, 6)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "134602752 42527232\n134602752 42527232\n"
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656477729916
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\r\n",
        "learning_rate = 3e-5\r\n",
        "optimizer = torch.optim.Adam(Cls_AraElectra.parameters(), lr=learning_rate)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "#criterion_span = nn.CrossEntropyLoss(reduction='none')\r\n",
        "cls_criterion = nn.CrossEntropyLoss(reduction='none')\r\n",
        "Cls_AraElectra.to(device)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "ElectraForSequenceClassification(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): ElectraClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656477734178
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_trained_model, cls_pred = cls_train(Cls_AraElectra, 0, 2, optimizer, 0, cls_train_loader, val_loader , True, 'third')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch 0: 100%|██████████| 9605/9605 [1:28:34<00:00,  1.81it/s, loss=0.339] \nEpoch 1: 100%|██████████| 9605/9605 [1:28:55<00:00,  1.80it/s, loss=0.562] \n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'acc': 79.40655946731567, 'train_loss': 0.6688395292835084}\n{'acc': 84.32066440582275, 'train_loss': 0.4508261918751091}\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656466468693
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_trained_model = cls_train(cls_trained_model, 2, 6, optimizer, 84.3, cls_train_loader, val_loader , True, 'third')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch 2: 100%|██████████| 9605/9605 [1:28:37<00:00,  1.81it/s, loss=0.109]  \nEpoch 3:  34%|███▍      | 3242/9605 [29:53<1:15:22,  1.41it/s, loss=0.00431]"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'acc': 85.14315485954285, 'train_loss': 0.32205695654617095}\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656474116012
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_trained_model, optimizer , x, xx = load_ckp('Runs/AraElectraDecoupledAsquadv2/train/cls/third/curr.pt',Cls_AraElectra, optimizer)"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656477773149
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_trained_model = cls_train(cls_trained_model, 3, 8, optimizer, 85.1, cls_train_loader, val_loader , True, 'third')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch 3: 100%|██████████| 9605/9605 [1:28:16<00:00,  1.81it/s, loss=0.0272]  \nEpoch 4: 100%|██████████| 9605/9605 [1:28:42<00:00,  1.80it/s, loss=0.079]   \nEpoch 5: 100%|██████████| 9605/9605 [1:28:24<00:00,  1.81it/s, loss=0.0175]  \nEpoch 6: 100%|██████████| 9605/9605 [1:28:25<00:00,  1.81it/s, loss=0.332]   \nEpoch 7:   2%|▏         | 174/9605 [01:35<1:26:23,  1.82it/s, loss=0.0748]  \n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'acc': 84.52889323234558, 'train_loss': 0.19359704868555624}\n{'acc': 84.82040762901306, 'train_loss': 0.1257312193015077}\n{'acc': 85.09109616279602, 'train_loss': 0.10057395500561123}\n{'acc': 84.21655297279358, 'train_loss': 0.15071689100924335}\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cls_trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mcls_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_trained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m85.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthird\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mcls_train\u001b[0;34m(model, start_epoch, num_epochs, optimizer, max_acc, train_loader, val_loader, log, exp_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mview(masks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m2\u001b[39m,)\n\u001b[1;32m     15\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_ans\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor)\n\u001b[0;32m---> 16\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m cls_criterion(pred, target\u001b[38;5;241m.\u001b[39mview(masks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],))\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m*\u001b[39m(weights\u001b[38;5;241m.\u001b[39mview(masks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656660426639
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Cls Model if needed"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_model = ElectraForSequenceClassification.from_pretrained(model_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652659508937
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-5\r\n",
        "optimizer = torch.optim.AdamW(cls_model.parameters(), lr=learning_rate)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "#criterion_span = nn.CrossEntropyLoss(reduction='none')\r\n",
        "#cls_criterion = nn.CrossEntropyLoss()\r\n",
        "cls_model.to(device)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652659514157
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_model, optimizer, start_epoch, result_dict = load_ckp('Runs/AraElectraDecoupledAsquadv2/train/cls/second/best.pt', cls_model, optimizer)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652659536222
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Span Training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_preds(data_loader, model,ids_to_index,offset,contexts, max_answer_length, n_best_size): \r\n",
        "  model.eval()\r\n",
        "  imd_predictions,script_predictions = dict(), dict()\r\n",
        "  with torch.no_grad():\r\n",
        "    #F1 = EM = Total = 0\r\n",
        "    total_loss = 0.0\r\n",
        "    total_predictions = dict()\r\n",
        "    no_probs_pred = dict()\r\n",
        "    #loop = tqdm(data_loader)\r\n",
        "    loop = tqdm(data_loader, leave=True)\r\n",
        "    for batch_idx, batch in enumerate(loop):\r\n",
        "      tokens = batch['input_ids'].to(device)\r\n",
        "      masks = batch['attention_mask'].to(device)\r\n",
        "      tokens_type = batch['token_type_ids'].to(device)\r\n",
        "      gt_start = batch['start_positions'].to(device)\r\n",
        "      gt_end = batch['end_positions'].to(device)\r\n",
        "      IDs = batch['IDs'].to(device)\r\n",
        "      outputs = model(tokens, masks, tokens_type, start_positions=gt_start, end_positions=gt_end)\r\n",
        "      #calculating loss\r\n",
        "      loss = outputs.loss\r\n",
        "      #update average total loss \r\n",
        "      total_loss = total_loss + ((1 / (batch_idx + 1)) * (loss.item() - total_loss)) \r\n",
        "      #calculating f1 score and EM\r\n",
        "      curr_batch_size = tokens.shape[0]\r\n",
        "      post_raw_preds(IDs, outputs.start_logits, outputs.end_logits, ids_to_index, offset, contexts,max_answer_length, n_best_size, imd_predictions, script_predictions )\r\n",
        "    #saving evaluation results\r\n",
        "    #evaluation\r\n",
        "\r\n",
        "    model.train()\r\n",
        "    return imd_predictions,script_predictions"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525181028
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_raw_preds(IDs, total_start_logits, total_end_logits,ids_to_index,offset,contexts, max_answer_length, n_best_size,\r\n",
        " imd_predictions,script_predictions ):\r\n",
        "    total_start_logits = total_start_logits.cpu().numpy()\r\n",
        "    total_end_logits = total_end_logits.cpu().numpy()\r\n",
        "    IDs = IDs.cpu().numpy()\r\n",
        "    for i in range(IDs.shape[0]):\r\n",
        "        offset_mapping = offset[ids_to_index[IDs[i].squeeze()]]\r\n",
        "        # The first feature comes from the first example. For the more general case, we will need to be match the example_id to\r\n",
        "        # an example index\r\n",
        "        context = contexts[ids_to_index[IDs[i].squeeze()]]\r\n",
        "        start_logits = total_start_logits[i]\r\n",
        "        end_logits = total_end_logits[i]\r\n",
        "        # Gather the indices the best start/end logits:\r\n",
        "        start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n",
        "        end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n",
        "        valid_answers = []\r\n",
        "        for start_index in start_indexes:\r\n",
        "            for end_index in end_indexes:\r\n",
        "                # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\r\n",
        "                # to part of the input_ids that are not in the context.\r\n",
        "                if (\r\n",
        "                    start_index >= len(offset_mapping)\r\n",
        "                    or end_index >= len(offset_mapping)\r\n",
        "                    or offset_mapping[start_index] is None\r\n",
        "                    or offset_mapping[end_index] is None\r\n",
        "                ):\r\n",
        "                    continue\r\n",
        "                # Don't consider answers with a length that is either < 0 or > max_answer_length.\r\n",
        "                if end_index < start_index or end_index - start_index + 1 > max_answer_length:\r\n",
        "                    continue\r\n",
        "                if start_index <= end_index: # We need to refine that test to check the answer is inside the context\r\n",
        "                    start_char = offset_mapping[start_index][0]\r\n",
        "                    end_char = offset_mapping[end_index][1]\r\n",
        "                    valid_answers.append(\r\n",
        "                        {\r\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\r\n",
        "                            \"text\": context[start_char: end_char]\r\n",
        "                        }\r\n",
        "                    )\r\n",
        "        if len(valid_answers) ==0:\r\n",
        "            valid_answers.append({\"text\":\"\", \"score\":\"\"})\r\n",
        "\r\n",
        "        valid_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\r\n",
        "        imd_predictions[str(IDs[i].squeeze())] = valid_answer\r\n",
        "        script_predictions[str(IDs[i].squeeze())] = valid_answer['text']"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525181330
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds(total_preds, no_probs_preds,data_path, log_path):\r\n",
        "    preds_path = os.path.join(log_path, 'preds')\r\n",
        "    if not os.path.exists(preds_path):\r\n",
        "        os.mkdir(preds_path)\r\n",
        "    no_probs_path = os.path.join(preds_path, 'na_probs.json')\r\n",
        "    text_preds_path = os.path.join(preds_path, 'preds.json')\r\n",
        "    jsonString = json.dumps(total_preds)\r\n",
        "    jsonFile = open(text_preds_path, \"w\")\r\n",
        "    jsonFile.write(jsonString)\r\n",
        "    jsonFile.close()\r\n",
        "    if no_probs_preds is not None:\r\n",
        "        jsonString = json.dumps(no_probs_preds)\r\n",
        "        jsonFile = open(no_probs_path, \"w\")\r\n",
        "        jsonFile.write(jsonString)\r\n",
        "        jsonFile.close()\r\n",
        "        #!python evaluatev2.py data_path text_preds_path electra --na-prob-file no_probs_path --na-prob-thresh 0.4 --out-file log_path\r\n",
        "        #os.system(f\"python evaluatev2.py {data_path} {text_preds_path} electra --na-prob-file {no_probs_path} --na-prob-thresh 0.5 --out-file {log_path}\")\r\n",
        "        !/anaconda/envs/azureml_py38/bin/python3 evaluatev2.py Data/asquadv2-val.json Runs/AraElectraDecoupledAsquadv2/train/span/fourth/preds/preds.json electra --na-prob-file Runs/AraElectraDecoupledAsquadv2/train/span/second/preds/na_probs.json  --na-prob-thresh 0.5  \r\n",
        "    else:\r\n",
        "        !/anaconda/envs/azureml_py38/bin/python3 evaluatev2.py Data/asquadv2-val.json Runs/AraElectraDecoupledAsquadv2/train/span/fourth/preds/preds.json electra  --out-file Runs/AraElectraDecoupledAsquadv2/train/span/fourth\r\n",
        "    if log_path:\r\n",
        "        with open(os.path.join(log_path, 'res.csv')) as f:\r\n",
        "            DictReader_obj = csv.DictReader(f)\r\n",
        "            lastrow = None\r\n",
        "            for item in DictReader_obj:\r\n",
        "                lastrow = dict(item)\r\n",
        "        #print(lastrow)\r\n",
        "        return lastrow\r\n",
        "    return 1\r\n"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525181550
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def span_train(model,start_epoch, num_epochs, optimizer,max_compined_metric, train_loader, val_loader, log, exp_name):\r\n",
        "  curr_ckp_path, best_ckp_path, exp_path = order_exp('Runs/AraElectraDecoupledAsquadv2/train/span', exp_name)\r\n",
        "  model.train()\r\n",
        "  for epoch in range(start_epoch,num_epochs):\r\n",
        "    total_loss = 0.0\r\n",
        "    loop = tqdm(train_loader, leave=True)\r\n",
        "    for batch_idx, batch in enumerate(loop):\r\n",
        "      tokens = batch['input_ids'].to(device)\r\n",
        "      masks = batch['attention_mask'].to(device)\r\n",
        "      tokens_type = batch['token_type_ids'].to(device)\r\n",
        "      gt_start = batch['start_positions'].to(device)\r\n",
        "      gt_end = batch['end_positions'].to(device)\r\n",
        "      outputs = model(tokens, masks, tokens_type, start_positions=gt_start, end_positions=gt_end)\r\n",
        "      loss = outputs.loss\r\n",
        "      loss = 2*loss\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      optimizer.zero_grad()\r\n",
        "      total_loss = total_loss + ((1 / (batch_idx + 1)) * (loss.item() - total_loss)) \r\n",
        "      loop.set_description(f'Epoch {epoch}')\r\n",
        "      loop.set_postfix(loss=loss.item())\r\n",
        "    \r\n",
        "    imd_preds, script_preds = get_raw_preds(val_loader, model,val_ids_to_idx,val_offset,asquad_val_contexts, 30, 10)\r\n",
        "    result_dict = get_preds(script_preds, None,'Data/asquadv2-val.json',exp_path )\r\n",
        "    checkpoint = {\r\n",
        "            'epoch': epoch + 1,\r\n",
        "            'result_dict':result_dict,\r\n",
        "            'state_dict': model.state_dict(),\r\n",
        "            'optimizer': optimizer.state_dict(),\r\n",
        "        }\r\n",
        "    curr_compined_metric = float(result_dict['HasAns_exact'])+1.5*float(result_dict['HasAns_f1'])\r\n",
        "    if curr_compined_metric>=max_compined_metric:\r\n",
        "      max_compined_metric = curr_compined_metric\r\n",
        "      save_ckp(checkpoint, True, curr_ckp_path, best_ckp_path)\r\n",
        "    else:\r\n",
        "      save_ckp(checkpoint, False, curr_ckp_path, best_ckp_path)\r\n",
        "    print(\"ckp saved\")\r\n",
        "  return model\r\n"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525181743
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QA_AraElectra = ElectraForQuestionAnswering.from_pretrained(model_name)\r\n",
        "freeze(QA_AraElectra.electra, 6)\r\n",
        "span_num_epochs = 4\r\n",
        "span_learning_rate = 3e-5\r\n",
        "span_optimizer = torch.optim.AdamW(QA_AraElectra.parameters(), lr=span_learning_rate)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "#criterion_span = nn.CrossEntropyLoss(reduction='none')\r\n",
        "#cls_criterion = nn.CrossEntropyLoss()\r\n",
        "QA_AraElectra.to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "134602752 42527232\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "ElectraForQuestionAnswering(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656516171945
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "span_trained_model = span_train(QA_AraElectra,0, 4, span_optimizer,0.0, span_train_loader, val_loader, True, 'fourth')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch 0: 100%|██████████| 5256/5256 [49:12<00:00,  1.78it/s, loss=4.75] \n100%|██████████| 1201/1201 [05:51<00:00,  3.42it/s]\nEpoch 1: 100%|██████████| 5256/5256 [49:04<00:00,  1.78it/s, loss=0.356] \n100%|██████████| 1201/1201 [05:50<00:00,  3.42it/s]\nEpoch 2:   2%|▏         | 129/5256 [01:13<1:01:37,  1.39it/s, loss=1.36]  \rEpoch 2:  44%|████▍     | 2338/5256 [21:57<25:31,  1.91it/s, loss=2.59]  \rEpoch 2:  50%|████▉     | 2615/5256 [24:29<21:29,  2.05it/s, loss=1.51] \rEpoch 2:  50%|████▉     | 2616/5256 [24:29<23:41,  1.86it/s, loss=1.51]"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"exact\": 31.754294638209267,\n  \"f1\": 38.97888823055597,\n  \"total\": 9605,\n  \"HasAns_exact\": 58.001902949571836,\n  \"HasAns_f1\": 71.20689276013132,\n  \"HasAns_total\": 5255,\n  \"NoAns_exact\": 0.04597701149425287,\n  \"NoAns_f1\": 0.04597701149425287,\n  \"NoAns_total\": 4350\n}\nckp saved\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"exact\": 32.181155648099946,\n  \"f1\": 39.32540306155587,\n  \"total\": 9605,\n  \"HasAns_exact\": 58.78211227402474,\n  \"HasAns_f1\": 71.84024669957071,\n  \"HasAns_total\": 5255,\n  \"NoAns_exact\": 0.04597701149425287,\n  \"NoAns_f1\": 0.04597701149425287,\n  \"NoAns_total\": 4350\n}\nckp saved\n"
        }
      ],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652672239995
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "span_trained_model, span_optim , x, xx = load_ckp('Runs/AraElectraDecoupledAsquadv2/train/span/fourth/curr.pt', QA_AraElectra,span_optimizer)"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656516539723
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_metric = float(xx['HasAns_exact'])+1.5*float(xx['HasAns_f1'])\r\n",
        "print(max_metric)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "166.5424823233808\n"
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656516547401
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "span_trained_model = span_train(span_trained_model,2, 4, span_optim,max_metric, span_train_loader, val_loader, True, 'fourth')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch 2: 100%|██████████| 5256/5256 [49:05<00:00,  1.78it/s, loss=0.583] \n100%|██████████| 1201/1201 [05:50<00:00,  3.43it/s]\nEpoch 3: 100%|██████████| 5256/5256 [49:04<00:00,  1.78it/s, loss=0.418] \n100%|██████████| 1201/1201 [05:50<00:00,  3.43it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"exact\": 31.952108276939093,\n  \"f1\": 39.32610496387188,\n  \"total\": 9605,\n  \"HasAns_exact\": 58.36346336822074,\n  \"HasAns_f1\": 71.84152962473632,\n  \"HasAns_total\": 5255,\n  \"NoAns_exact\": 0.04597701149425287,\n  \"NoAns_f1\": 0.04597701149425287,\n  \"NoAns_total\": 4350\n}\nckp saved\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"exact\": 32.02498698594482,\n  \"f1\": 39.26325553393223,\n  \"total\": 9605,\n  \"HasAns_exact\": 58.49666983824929,\n  \"HasAns_f1\": 71.72665450112638,\n  \"HasAns_total\": 5255,\n  \"NoAns_exact\": 0.04597701149425287,\n  \"NoAns_f1\": 0.04597701149425287,\n  \"NoAns_total\": 4350\n}\nckp saved\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656523160896
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Test Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Pretrained models"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels = 2)\r\n",
        "learning_rate = 3e-5\r\n",
        "optimizer = torch.optim.AdamW(cls_model.parameters(), lr=learning_rate)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "#criterion_span = nn.CrossEntropyLoss(reduction='none')\r\n",
        "#cls_criterion = nn.CrossEntropyLoss()\r\n",
        "cls_model.to(device)\r\n",
        "cls_model , cls_optim, x, xx = load_ckp('Runs/AraElectraDecoupledAsquadv2/train/cls/third/best.pt', cls_model,optimizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForSequenceClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656516232917
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_model.save_pretrained('AraElectra-ASQuADv2-CLS')\r\n",
        "araelectra_tokenizer.save_pretrained('AraElectra-ASQuADv2-CLS')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 39,
          "data": {
            "text/plain": "('AraElectra-ASQuADv2-CLS/tokenizer_config.json',\n 'AraElectra-ASQuADv2-CLS/special_tokens_map.json',\n 'AraElectra-ASQuADv2-CLS/vocab.txt',\n 'AraElectra-ASQuADv2-CLS/added_tokens.json')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656516376370
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = ElectraForQuestionAnswering.from_pretrained(model_name)\r\n",
        "span_learning_rate = 3e-5\r\n",
        "span_optimizer = torch.optim.AdamW(qa_model.parameters(), lr=span_learning_rate)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "qa_model.to(device)\r\n",
        "qa_model , qa_optim, x, xx = load_ckp('Runs/AraElectraDecoupledAsquadv2/train/span/fourth/best.pt', qa_model,span_optimizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656523324927
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model.save_pretrained('AraElectra-ASQuADv2-QA')\r\n",
        "araelectra_tokenizer.save_pretrained('AraElectra-ASQuADv2-QA')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": "('AraElectra-ASQuADv2-QA/tokenizer_config.json',\n 'AraElectra-ASQuADv2-QA/special_tokens_map.json',\n 'AraElectra-ASQuADv2-QA/vocab.txt',\n 'AraElectra-ASQuADv2-QA/added_tokens.json')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656523329503
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on Test Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = ElectraForQuestionAnswering.from_pretrained('AraElectra-ASQuADv2-QA')\r\n",
        "cls_model = ElectraForSequenceClassification.from_pretrained('AraElectra-ASQuADv2-CLS',num_labels=2)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "qa_model.to(device)\r\n",
        "cls_model.to(device)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "ElectraForSequenceClassification(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): ElectraClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525216362
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_res_dict = cls_eval(cls_model, test_loader,'Runs/AraElectraDecoupledAsquadv2/train/cls/third/test-eval',None)"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525713950
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_res_dict"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "{'acc': 84.53930020332336, 'train_loss': None}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656525999768
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_preds_dict = dict()\r\n",
        "with open('Runs/AraElectraDecoupledAsquadv2/train/cls/third/test-eval/preds.txt','r') as f:\r\n",
        "    for line in f:\r\n",
        "        line = line.strip()\r\n",
        "        elements = line.split(',')\r\n",
        "        cls_preds_dict[elements[0]] = float(elements[1])\r\n",
        "print(len(cls_preds_dict))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "9606\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656526588284
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imd_preds, script_preds = get_raw_preds(test_loader, qa_model,test_ids_to_idx,test_offset,asquad_test_contexts, 30, 10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 1201/1201 [05:47<00:00,  3.45it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTraceback (most recent call last):\n  File \"evaluatev2.py\", line 295, in <module>\n    main()\n  File \"evaluatev2.py\", line 240, in main\n    with open(OPTS.na_prob_file) as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'Runs/AraElectraDecoupledAsquadv2/train/span/second/preds/na_probs.json'\n"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Runs/AraElectraDecoupledAsquadv2/train/span/fourth/test-eval/res.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m imd_preds, script_preds \u001b[38;5;241m=\u001b[39m get_raw_preds(test_loader, qa_model,test_ids_to_idx,test_offset,asquad_test_contexts, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m qa_result_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_preds_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData/asquadv2-val.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRuns/AraElectraDecoupledAsquadv2/train/span/fourth/test-eval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mget_preds\u001b[0;34m(total_preds, no_probs_preds, data_path, log_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/anaconda/envs/azureml_py38/bin/python3 evaluatev2.py Data/asquadv2-val.json Runs/AraElectraDecoupledAsquadv2/train/span/fourth/preds/preds.json electra  --out-file Runs/AraElectraDecoupledAsquadv2/train/span/fourth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_path:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mres.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     23\u001b[0m         DictReader_obj \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(f)\n\u001b[1;32m     24\u001b[0m         lastrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Runs/AraElectraDecoupledAsquadv2/train/span/fourth/test-eval/res.csv'"
          ]
        }
      ],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656660432115
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_result_dict = get_preds2(script_preds, cls_preds_dict,'Data/asquadv2-test.json','Runs/AraElectraDecoupledAsquadv2/train/span/fourth/test-eval' )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"exact\": 65.11555277951281,\n  \"f1\": 71.49042547237256,\n  \"total\": 9606,\n  \"HasAns_exact\": 56.14535768645358,\n  \"HasAns_f1\": 67.79623803036668,\n  \"HasAns_total\": 5256,\n  \"NoAns_exact\": 75.95402298850574,\n  \"NoAns_f1\": 75.95402298850574,\n  \"NoAns_total\": 4350,\n  \"best_exact\": 67.7597334998959,\n  \"best_exact_thresh\": 0.12791500985622406,\n  \"best_f1\": 73.3514620564771,\n  \"best_f1_thresh\": 0.20527765154838562\n}\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656527588202
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds2(total_preds, no_probs_preds,data_path, log_path):\r\n",
        "    preds_path = os.path.join(log_path, 'preds')\r\n",
        "    if not os.path.exists(preds_path):\r\n",
        "        os.mkdir(preds_path)\r\n",
        "    no_probs_path = os.path.join(preds_path, 'na_probs.json')\r\n",
        "    text_preds_path = os.path.join(preds_path, 'preds.json')\r\n",
        "    jsonString = json.dumps(total_preds)\r\n",
        "    jsonFile = open(text_preds_path, \"w\")\r\n",
        "    jsonFile.write(jsonString)\r\n",
        "    jsonFile.close()\r\n",
        "    if no_probs_preds is not None:\r\n",
        "        jsonString = json.dumps(no_probs_preds)\r\n",
        "        jsonFile = open(no_probs_path, \"w\")\r\n",
        "        jsonFile.write(jsonString)\r\n",
        "        jsonFile.close()\r\n",
        "        #!python evaluatev2.py data_path text_preds_path electra --na-prob-file no_probs_path --na-prob-thresh 0.4 --out-file log_path\r\n",
        "        #os.system(f\"python evaluatev2.py {data_path} {text_preds_path} electra --na-prob-file {no_probs_path} --na-prob-thresh 0.5 --out-file {log_path}\")\r\n",
        "        !/anaconda/envs/azureml_py38/bin/python3 evaluatev2.py Data/asquadv2-test.json Runs/AraElectraDecoupledAsquadv2/train/span/fourth/test-eval/preds/preds.json electra --na-prob-file Runs/AraElectraDecoupledAsquadv2/train/span/fourth/test-eval/preds/na_probs.json  --na-prob-thresh 0.5  \r\n",
        "    return 1"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1656527580966
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}