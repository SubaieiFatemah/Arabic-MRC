{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import shutil\r\n",
        "from collections import Counter, dqueue\r\n",
        "import numpy as np\r\n",
        "import json\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from transformers import AutoModel , AutoTokenizer, ElectraForQuestionAnswering\r\n",
        "from Preprocess.arabertpreprocess import ArabertPreprocessor\r\n",
        "torch.manual_seed(3407)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "<torch._C.Generator at 0x7f203c10adb0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961164082
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_end_index(answer, context):\r\n",
        "  ## 1 if span mathc the context 0 otherwise\r\n",
        "  text = answer['text']\r\n",
        "  start_idx = answer['answer_start']\r\n",
        "  end_idx = start_idx + len(text)\r\n",
        "  if text == context[start_idx:end_idx]:\r\n",
        "    answer['answer_end'] = end_idx\r\n",
        "    return False\r\n",
        "  for i in range(1,3):\r\n",
        "    if text == context[start_idx-i:end_idx-i]:\r\n",
        "      answer['answer_end']= end_idx-1\r\n",
        "      answer['answer_start'] = start_idx-1\r\n",
        "      return False\r\n",
        "  return True"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961165092
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def electra_preprocess(context, question, answer, electraprep):\r\n",
        "    answer['text'] = electraprep.preprocess(answer['text'][0])\r\n",
        "    context = electraprep.preprocess(context)\r\n",
        "    question = electraprep.preprocess(question)\r\n",
        "    res = context.find(answer['text'])\r\n",
        "    answer['answer_start'] = res\r\n",
        "    return context, question ,answer, res"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961443867
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_examples(filepath):\r\n",
        "        \"\"\"This function returns the examples in the raw (text) form.\"\"\"\r\n",
        "        #logger.info(\"generating examples from = %s\", filepath)\r\n",
        "        with open(filepath, encoding=\"utf-8\") as f:\r\n",
        "            arcd = json.load(f)\r\n",
        "            for article in arcd[\"data\"]:\r\n",
        "                title = article.get(\"title\", \"\").strip()\r\n",
        "                for paragraph in article[\"paragraphs\"]:\r\n",
        "                    context = paragraph[\"context\"].strip()\r\n",
        "                    for qa in paragraph[\"qas\"]:\r\n",
        "                        question = qa[\"question\"].strip()\r\n",
        "                        id_ = qa[\"id\"]\r\n",
        "\r\n",
        "                        answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\r\n",
        "                        answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\r\n",
        "\r\n",
        "                        # Features currently used are \"context\", \"question\", and \"answers\".\r\n",
        "                        # Others are extracted here for the ease of future expansions.\r\n",
        "                        yield id_, {\r\n",
        "                            \"title\": title,\r\n",
        "                            \"context\": context,\r\n",
        "                            \"question\": question,\r\n",
        "                            \"id\": id_,\r\n",
        "                            \"answers\": {\"answer_start\": answer_starts, \"text\": answers},\r\n",
        "                        }"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961445129
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Read_ARCD(path, electraprep):\r\n",
        "  contexts, questions, answers = [], [], []\r\n",
        "  for x, data in generate_examples(path):\r\n",
        "    #print(data['answers']['text'][0])\r\n",
        "    context, question, answer, res = electra_preprocess(data['context'],data['question'], data['answers'], electraprep)\r\n",
        "    #print(answer, context)\r\n",
        "    #print(res)\r\n",
        "    if res==-1:\r\n",
        "        continue\r\n",
        "    add_end_index(answer, context)\r\n",
        "    contexts.append(context)\r\n",
        "    questions.append(question)\r\n",
        "    answers.append(answer)\r\n",
        "  return contexts, questions, answers\r\n",
        "\r\n",
        "model_name = 'araelectra-base-discriminator'\r\n",
        "electraprep= ArabertPreprocessor(model_name=model_name)\r\n",
        "train_contexts, train_questions, train_answers = Read_ARCD('Data/arcd-train.json', electraprep)\r\n",
        "val_contexts, val_questions, val_answers = Read_ARCD('Data/arcd-test.json', electraprep)"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961493119
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_contexts))\r\n",
        "print(len(val_contexts))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "693\n702\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961501111
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encodings"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the tokenizer\r\n",
        "model_name = 'aubmindlab/araelectra-base-discriminator'\r\n",
        "electra_tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case=False)\r\n",
        "train_encodings = electra_tokenizer(train_questions, train_contexts, truncation=True, padding=True, return_tensors=\"pt\")\r\n",
        "val_encodings = electra_tokenizer(val_questions, val_contexts, truncation=True, padding=True, return_tensors=\"pt\")\r\n",
        "#test_encodings = electra_tokenizer(test_questions, test_contexts,truncation= True, padding= True, return_tensors=\"pt\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/503 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34c24eb4127a425da974045e03857fdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/825k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67b7c0ee54af4c3191ea90e3c20d9c12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/2.64M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8f2899ac49b4cdc95de8c54836633f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a4338c149a544908feedee9c3467983"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/392 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00ba64d8065542fca8c06d572e3216ee"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961622787
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_encodings.input_ids.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "torch.Size([702, 472])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961651260
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index_to_token_position(encodings , answers):\r\n",
        "  start_positions = list()\r\n",
        "  end_positions = list()\r\n",
        "  for i in range(len(answers)):\r\n",
        "    start_positions.append(encodings.char_to_token(i, answers[i]['answer_start'], 1))\r\n",
        "    end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'], 1))\r\n",
        "    #if context truncated\r\n",
        "    if start_positions[-1] is None: \r\n",
        "      start_positions[-1] = electra_tokenizer.model_max_length\r\n",
        "    #if end index is space\r\n",
        "    itt = 1\r\n",
        "    while end_positions[-1] is None: \r\n",
        "      end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end']-itt, 1)\r\n",
        "      itt = itt + 1 \r\n",
        "  encodings.update({'start_positions': torch.tensor(start_positions), 'end_positions': torch.tensor(end_positions)})\r\n",
        "  encodings['start_positions'] = encodings['start_positions'].view(len(answers), 1)\r\n",
        "  encodings['end_positions'] = encodings['end_positions'].view(len(answers), 1)"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961701066
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_token_position(train_encodings, train_answers)\r\n",
        "index_to_token_position(val_encodings, val_answers)"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961724409
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_truncated(start_pos):\r\n",
        "  cnt = 0\r\n",
        "  for pos in start_pos:\r\n",
        "    if pos==512:\r\n",
        "      cnt+=1\r\n",
        "  return cnt\r\n",
        "\r\n",
        "print(is_truncated(train_encodings['start_positions']))\r\n",
        "print(is_truncated(val_encodings['start_positions']))\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0\n0\n"
        }
      ],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961760913
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and DataLoader"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from tqdm import tqdm"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961789898
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AqadDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, encodings):\r\n",
        "        self.encodings = encodings\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.encodings.input_ids)\r\n",
        "\r\n",
        "train_dataset = AqadDataset(train_encodings)\r\n",
        "val_dataset = AqadDataset(val_encodings)"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961798253
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\r\n",
        "val_loader = DataLoader(val_dataset, batch_size = 8, shuffle= True)"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961808868
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EM_score(pred, GT):\r\n",
        "  if torch.equal(pred, GT):return 1\r\n",
        "  return 0"
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961820443
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def F1_score(prediction, ground_truth):\r\n",
        "    prediction_tokens = prediction.tolist()\r\n",
        "    ground_truth_tokens = ground_truth.tolist()\r\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\r\n",
        "    num_same = sum(common.values())\r\n",
        "    if num_same == 0:\r\n",
        "        return 0\r\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\r\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\r\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\r\n",
        "    return f1"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961831150
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\r\n",
        "    \"\"\"\r\n",
        "    state: checkpoint to save\r\n",
        "    is_best: is this the best checkpoint; min validation loss\r\n",
        "    checkpoint_path: path to save checkpoint\r\n",
        "    best_model_path: path to save best checkpoint\r\n",
        "    \"\"\"\r\n",
        "    f_path = checkpoint_path\r\n",
        "    # save checkpoint data to the path given, checkpoint_path\r\n",
        "    torch.save(state, f_path)\r\n",
        "    # if it is a best model, min validation loss\r\n",
        "    if is_best:\r\n",
        "        best_fpath = best_model_path\r\n",
        "        # copy that checkpoint file to best path given, best_model_path\r\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "outputs": [],
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961841913
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\r\n",
        "    \"\"\"\r\n",
        "    checkpoint_path: path to saved checkpoint\r\n",
        "    model: model to load checkpoint parameters into       \r\n",
        "    optimizer: optimizer defined in previous training\r\n",
        "    \"\"\"\r\n",
        "    # load check point\r\n",
        "    checkpoint = torch.load(checkpoint_fpath)\r\n",
        "    # initialize state_dict from checkpoint to model\r\n",
        "    model.load_state_dict(checkpoint['state_dict'])\r\n",
        "    # initialize optimizer from checkpoint to optimizer\r\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\r\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\r\n",
        "    valid_loss_min = checkpoint['val_loss']\r\n",
        "    # return model, optimizer, epoch value, min validation loss \r\n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min"
      ],
      "outputs": [],
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961853333
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def order_exp(base_path, exp_name):\r\n",
        "  exp_path = os.path.join(base_path, exp_name)\r\n",
        "  if not os.path.exists(exp_path):\r\n",
        "    os.mkdir(exp_path)\r\n",
        "  curr_ckp_path = os.path.join(exp_path,'curr.pt')\r\n",
        "  best_ckp_path = os.path.join(exp_path, 'best.pt')\r\n",
        "  return curr_ckp_path, best_ckp_path, exp_path"
      ],
      "outputs": [],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645961862399
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'aubmindlab/araelectra-base-discriminator'\r\n",
        "QA_AraElectra = ElectraForQuestionAnswering.from_pretrained(model_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at aubmindlab/araelectra-base-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at aubmindlab/araelectra-base-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645962112512
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader, model, log, log_path=None, train_loss=None): \r\n",
        "  model.eval()\r\n",
        "  with torch.no_grad():\r\n",
        "    F1 = EM = Total = 0\r\n",
        "    total_loss = 0.0\r\n",
        "    #loop = tqdm(data_loader)\r\n",
        "    #loop = tqdm(data_loader, leave=True)\r\n",
        "    for batch_idx, batch in enumerate(data_loader):\r\n",
        "      #moving tensors to gpu\r\n",
        "      #print(f\"this is batch size {data_loader.batch_size}\")\r\n",
        "      \r\n",
        "      tokens = batch['input_ids'].to(device)\r\n",
        "      masks = batch['attention_mask'].to(device)\r\n",
        "      tokens_type = batch['token_type_ids'].to(device)\r\n",
        "      gt_start = batch['start_positions'].to(device)\r\n",
        "      gt_end = batch['end_positions'].to(device)\r\n",
        "      #weights = batch['weights'].to(device)\r\n",
        "      #print(f\"this is tensor size {gt_start.shape}\")\r\n",
        "      #predictions\r\n",
        "      outputs = model(tokens, masks, tokens_type, start_positions=gt_start, end_positions=gt_end)\r\n",
        "      #calculating loss\r\n",
        "      loss = outputs.loss\r\n",
        "      #update average total loss \r\n",
        "      total_loss = total_loss + ((1 / (batch_idx + 1)) * (loss - total_loss)) \r\n",
        "      #calculating f1 score and EM\r\n",
        "      curr_batch_size = gt_start.shape[0]\r\n",
        "      #print(curr_batch_size)\r\n",
        "      for i in range(curr_batch_size):\r\n",
        "        #print(f\"this is tensor index {i}\")\r\n",
        "        start_gt, end_gt = batch['start_positions'][i], batch['end_positions'][i]\r\n",
        "        gt_tokens = batch['input_ids'][i][start_gt.item():end_gt.item()+1]\r\n",
        "        start_pred, end_pred = torch.argmax(outputs.start_logits[i],dim=0), torch.argmax(outputs.end_logits[i],dim =0)\r\n",
        "        pred_tokens = batch['input_ids'][i][start_pred.item():end_pred.item()+1]\r\n",
        "        F1 += F1_score(pred_tokens, gt_tokens)\r\n",
        "        EM += EM_score(torch.tensor([start_pred, end_pred]), torch.tensor([start_gt,end_gt]))\r\n",
        "        Total +=1\r\n",
        "    EM = 100.0 *EM/Total\r\n",
        "    F1 = 100.0 * F1 /Total\r\n",
        "    #saving evaluation results\r\n",
        "    \r\n",
        "    if(log):\r\n",
        "      log_path = os.path.join(log_path,'res.txt')\r\n",
        "      if not os.path.exists(log_path):\r\n",
        "          with open(log_path,'w') as f:\r\n",
        "              f.write('EM,f1,ValidationLoss,TrainLoss')\r\n",
        "      with open(log_path, 'a') as f:\r\n",
        "        #validation resultss\r\n",
        "        f.write(f\"{EM:.2f},{F1:.2f},{total_loss:.2f},{train_loss:.2f} \\n\") \r\n",
        "    model.train()\r\n",
        "    print(f\"Validation Results: EM:{EM:.2f}, f1: {F1:.2f}, loss: {total_loss:.2f}\")\r\n",
        "    return EM, F1, total_loss"
      ],
      "outputs": [],
      "execution_count": 67,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645963722085
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,start_epoch, num_epochs, optimizer,min_val_loss, train_loader, val_loader, log, exp_name):\r\n",
        "  curr_ckp_path, best_ckp_path, exp_path = order_exp('Runs/AraElectraArcd/train', exp_name)\r\n",
        "  model.train()\r\n",
        "  for epoch in range(start_epoch,num_epochs):\r\n",
        "    total_loss = 0.0\r\n",
        "    loop = tqdm(train_loader, leave=True)\r\n",
        "    for batch_idx, batch in enumerate(loop):\r\n",
        "      tokens = batch['input_ids'].to(device)\r\n",
        "      masks = batch['attention_mask'].to(device)\r\n",
        "      tokens_type = batch['token_type_ids'].to(device)\r\n",
        "      gt_start = batch['start_positions'].to(device)\r\n",
        "      gt_end = batch['end_positions'].to(device)\r\n",
        "      outputs = model(tokens, masks, tokens_type, start_positions=gt_start, end_positions=gt_end)\r\n",
        "      loss = outputs.loss\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      optimizer.zero_grad()\r\n",
        "      total_loss = total_loss + ((1 / (batch_idx + 1)) * (loss - total_loss)) \r\n",
        "      loop.set_description(f'Epoch {epoch}')\r\n",
        "      loop.set_postfix(loss=loss.item())\r\n",
        "\r\n",
        "    val_em, val_f1, val_loss = evaluate(val_loader, model , log, exp_path, total_loss)\r\n",
        "    checkpoint = {\r\n",
        "            'epoch': epoch + 1,\r\n",
        "            'val_loss': val_loss,\r\n",
        "            'val_em': val_em,\r\n",
        "            'val_f1': val_f1,\r\n",
        "            'train_loss':total_loss,\r\n",
        "            'state_dict': model.state_dict(),\r\n",
        "            'optimizer': optimizer.state_dict(),\r\n",
        "        }\r\n",
        "    if val_loss<=min_val_loss:\r\n",
        "      min_val_loss = val_loss\r\n",
        "      save_ckp(checkpoint, True, curr_ckp_path, best_ckp_path)\r\n",
        "    else:\r\n",
        "      save_ckp(checkpoint, False, curr_ckp_path, best_ckp_path)\r\n",
        "  return model\r\n"
      ],
      "outputs": [],
      "execution_count": 64,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645963460826
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\r\n",
        "learning_rate = 3e-5\r\n",
        "optimizer = torch.optim.Adam(QA_AraElectra.parameters(), lr=learning_rate)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "QA_AraElectra.to(device)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 65,
          "data": {
            "text/plain": "ElectraForQuestionAnswering(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 65,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645963461390
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train(QA_AraElectra,0, num_epochs, optimizer,np.inf, train_loader, val_loader, True, 'first')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch 0: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=2.7]  \nEpoch 1: 100%|██████████| 87/87 [02:16<00:00,  1.57s/it, loss=0.767]\nEpoch 2: 100%|██████████| 87/87 [02:16<00:00,  1.57s/it, loss=1.16] \nEpoch 3: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.11]  \nEpoch 4: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.122] \nEpoch 5: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.942] \nEpoch 6: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.157] \nEpoch 7: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0696]\nEpoch 8: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0236] \nEpoch 9: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0968] \nEpoch 10: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0261] \nEpoch 11: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0446] \nEpoch 12: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.00361]\nEpoch 13: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.00398]\nEpoch 14: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0036] \nEpoch 15: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.217]  \nEpoch 16: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.00992]\nEpoch 17: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.00474]\nEpoch 18: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0301] \nEpoch 19: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.00751]\nEpoch 20: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.00194]\nEpoch 21: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.00992] \nEpoch 22: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0462] \nEpoch 23: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.115]   \nEpoch 24: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.006]  \nEpoch 25: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0748] \nEpoch 26: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.00656]\nEpoch 27: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.419]  \nEpoch 28: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0102] \nEpoch 29: 100%|██████████| 87/87 [02:17<00:00,  1.58s/it, loss=0.0484]  \n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validation Results: EM:28.49, f1: 59.56, loss: 2.07\nValidation Results: EM:30.06, f1: 60.22, loss: 2.12\nValidation Results: EM:28.63, f1: 61.03, loss: 2.33\nValidation Results: EM:30.20, f1: 61.92, loss: 2.40\nValidation Results: EM:29.63, f1: 59.77, loss: 2.71\nValidation Results: EM:31.34, f1: 61.47, loss: 2.72\nValidation Results: EM:30.48, f1: 60.96, loss: 2.95\nValidation Results: EM:29.63, f1: 60.47, loss: 2.99\nValidation Results: EM:30.48, f1: 61.26, loss: 3.09\nValidation Results: EM:30.77, f1: 61.64, loss: 3.13\nValidation Results: EM:32.62, f1: 62.49, loss: 3.14\nValidation Results: EM:29.20, f1: 61.27, loss: 3.29\nValidation Results: EM:30.91, f1: 61.62, loss: 3.27\nValidation Results: EM:29.91, f1: 61.21, loss: 3.44\nValidation Results: EM:32.76, f1: 63.06, loss: 3.36\nValidation Results: EM:28.92, f1: 60.83, loss: 3.46\nValidation Results: EM:29.06, f1: 60.56, loss: 3.36\nValidation Results: EM:30.48, f1: 62.68, loss: 3.57\nValidation Results: EM:30.06, f1: 60.11, loss: 3.36\nValidation Results: EM:31.05, f1: 62.28, loss: 3.56\nValidation Results: EM:31.77, f1: 62.49, loss: 3.60\nValidation Results: EM:29.77, f1: 60.55, loss: 3.68\nValidation Results: EM:31.77, f1: 61.74, loss: 3.74\nValidation Results: EM:26.35, f1: 60.52, loss: 3.67\nValidation Results: EM:26.50, f1: 57.39, loss: 3.76\nValidation Results: EM:31.05, f1: 61.19, loss: 3.44\nValidation Results: EM:30.20, f1: 62.40, loss: 3.39\nValidation Results: EM:30.06, f1: 58.94, loss: 3.17\nValidation Results: EM:30.91, f1: 60.26, loss: 3.60\nValidation Results: EM:27.07, f1: 59.55, loss: 3.42\n"
        }
      ],
      "execution_count": 68,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1645969654745
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}